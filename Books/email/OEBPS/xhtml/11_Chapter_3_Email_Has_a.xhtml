<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>

    <title>Chapter 3: Email Has a Mind of Its Own, A World Without Email</title>
    <meta content="urn:uuid:fdcd3175-275e-4545-9822-dd3618c31f20" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link rel="stylesheet" type="text/css" href="../../stylesheet.css"/>
<link rel="stylesheet" type="text/css" href="../../page_styles.css"/>

  


<link href="../../calibreHtmlOutBasicCss.css" type="text/css" rel="stylesheet" />

</head>
<body>

<div class="calibreMeta">
  <div class="calibreMetaTitle">
  
  
    
    <h1>
      <a href="../../../n6gwhr7i.html">A World Without Email: Reimagining Work in an Age of Communication Overload
</a>
    </h1>
    
    
  
  </div>
  <div class="calibreMetaAuthor">
    Cal Newport

  </div>
</div>

<div class="calibreMain">

  <div class="calibreEbookContent">
    
      <div class="calibreEbNavTop">
        
          <a href="10_Chapter_2_Email_Makes.xhtml" class="calibreAPrev">previous page
</a>
        

        
          <a href="12_Part_2_Principles_for.xhtml" class="calibreANext">next page
</a>
        
      </div>
    

    
		<div class="calibre1">
			<h2 class="x03-chapter-number" id="_idParaDest-14"><span epub:type="pagebreak" id="page_63" role="doc-pagebreak" title="63" class="calibre4"></span>Chapter 3</h2>
			<h2 class="x03-chapter-title" id="_idParaDest-15">Email Has a Mind of Its Own</h2>
			<h3 class="x05-head-a">The Rise of Email</h3>
			<p class="x03-co-body-text">Why did email become so popular? One clue can be found in an unlikely place: hidden behind the walls of the Central Intelligence Agency’s original headquarters building in Langley, Virginia. Here you’ll find more than thirty miles of four-inch steel tubing, installed in the early 1960s, as part of an elaborate, vacuum-powered intra-office mail system. Messages, sealed in fiberglass containers, rocketed at thirty feet a second among approximately 150 stations spread over eight floors. Senders specified each capsule’s destination by manipulating brass rings at its base; electromechanical widgets in the tubes read those settings and routed the capsule. At its peak, the system delivered 7,500 messages each day.<a href="19_Notes.xhtml#EndnoteNumber68" id="SuperscriptNumber68" class="calibre3"><sup class="endnote">1</sup></a></p>
			<p class="x04-body-text">According to oral histories maintained by the CIA, employees were saddened when, in the late 1980s, during an expansion of the headquarters, this steampunk messaging system was shut down. Some of <span epub:type="pagebreak" id="page_64" role="doc-pagebreak" title="64"></span>them reminisced about the comforting <i class="calibre2">thunk</i>, <i class="calibre2">thunk</i> of the capsules arriving at a station; others worried that internal office communication would become unacceptably slow, or that runners would wear themselves out delivering messages on foot. The agency’s archives contain a photograph of a pin that reads “Save the Tubes.”</p>
			<p class="x04-body-text">Why would the CIA invest the significant amount of resources required to build and maintain such an unwieldy system? By the mid-twentieth century, much more common and inexpensive methods for office communication had already become standard. When this headquarters was built, for example, internal telephone exchanges had been around for decades. Isn’t it unnecessary to send you a note through a pneumatic tube network when I could just as easily call you directly using the telephone on my desk?</p>
			<p class="x04-body-text">But the telephone was no panacea. It represents an example of what communication specialists call <i class="calibre2">synchronous messaging</i>, which requires all parties in the interaction to participate at the same time. If you’re not at your desk when I dial your extension, or if your line is busy, then the attempted interaction is a bust. In a small organization, tracking people down on the phone might be manageable, but as the nineteenth century gave way to the twentieth, single-room countinghouses and small managerial suites tucked in the backs of factories gave way to huge edifices, like the CIA headquarters, that could house thousands of white-collar employees under the same roof. At this scale, the overhead of arranging synchronous communication becomes onerous, leading to drawn-out games of secretarial phone tag and piles of missed-call message slips.</p>
			<p class="x04-body-text">An alternative form of interaction that avoids the overhead problem is <i class="calibre2">asynchronous messaging</i>, which doesn’t require a receiver to be present when a message is sent. The intra-office mail cart is a classic example of this communication type. If I want to send you a note, I can <span epub:type="pagebreak" id="page_65" role="doc-pagebreak" title="65"></span>drop it in my outgoing mail tray when it’s convenient for me, and once it’s delivered to your incoming mail tray, you can pick it up and read it when convenient for you—all with no coordination between us required. The problem with the mail cart, of course, is that it’s slow. It might take the better part of a day for my note to actually make it from my outbox to a sorting station, then on to a cart on your floor, where eventually it will be pushed past your desk and manually delivered. This might be fine for conveying static information, but it’s clearly an impractical means to efficiently coordinate or share time-sensitive news.</p>
			<p class="x04-body-text">What the rise of the large office really needed—a productivity silver bullet of sorts—was some way to combine the <i class="calibre2">speed</i> of synchronous communication with the <i class="calibre2">low overhead</i> of asynchronous communication. Which brings us back to the CIA. This is exactly what they were trying to achieve with their pneumatic tube system. Their electromechanically routed, vacuum-driven capsules were the equivalent of a turbocharged mail cart: I can now asynchronously deliver you a message within minutes instead of hours. It’s not surprising, therefore, that the CIA employees were saddened to see the tube system shut down when the headquarters was expanded in the 1980s. But this sadness didn’t last long, as this same period marked the arrival of a newer, cheaper, even faster method for practical asynchronous messaging: electronic mail.<a href="19_Notes.xhtml#EndnoteNumber69" id="SuperscriptNumber69" class="calibre3"><sup class="endnote">2</sup></a></p>
			<hr class="transition"/>
			<div aria-hidden="true" class="x04-space-break-orn">—</div>
			<p class="x03-co-body-text">Most organizations lacked the resources to build a system similar to the CIA’s tubes, so for them, the arrival of email was the first time they could enjoy high-speed asynchrony. We’re so familiar with this capability today that we take it for granted, but during the 1980s and 1990s, when it began to spread widely, its impact was profound.</p>
			<p class="x04-body-text">We can find nice snapshots of email’s rapid ascendency in the <span epub:type="pagebreak" id="page_66" role="doc-pagebreak" title="66"></span>archives of <i class="calibre2">The</i> <i class="calibre2">New York Times</i> from this period. One of the paper’s earliest mentions of this technology in a business context is in a 1987 article that places the word <i class="calibre2">e-mail</i> in quotation marks throughout.<a href="19_Notes.xhtml#EndnoteNumber70" id="SuperscriptNumber70" class="calibre3"><sup class="endnote">3</sup></a> “Although ‘e-mail,’ as it is called, has not spread as rapidly as its proponents predicted,” it explains, “it has established itself as a niche market, and it has a small but increasing following in the corporate world.” As the article clarifies, professional email at this point still required a special application that would dial into a server to establish a connection, allowing you to send and receive messages before disconnecting. If you needed to reference the information from a message later, a laborious process was required to save it to a disk. Given the complexity of this technology at this early stage, the article’s caution about its importance is understandable. But this soon changed.</p>
			<p class="x04-body-text">Appearing just a few years later is another instructive article—this time without quotation marks around <i class="calibre2">e-mail.</i><a href="19_Notes.xhtml#EndnoteNumber71" id="SuperscriptNumber71" class="calibre3"><sup class="endnote">4</sup></a> The article describes the embrace of this technology within the entertainment industry. In 1989, we learn, Mike Simpson, the cohead of the powerful motion picture department at the William Morris Agency, connected three hundred computers in their Beverly Hills and New York offices with an early computer network technology offered by Steve Jobs’s post-Apple start-up, NeXT, Inc. “A cornerstone of our business is the quicker you get information, the quicker you can use it,” Simpson says. “E-mail has already given us an edge.”</p>
			<p class="x04-body-text">The article contains other examples of early admiration for email’s potential. “It’s fast information, replaces telephone calls, is environmentally correct and allows more people to know things at the same time,” explains one agent. Another talks about his experience shifting to the rival Creative Artists Agency, where, to his “horror,” he discovered that they were still delivering paper notes with runners. He insisted his new <span epub:type="pagebreak" id="page_67" role="doc-pagebreak" title="67"></span>colleagues adopt email. We also learn that at Disney, Jeffrey Katzenberg set up a private email network connecting twenty high-level executives. “We had to love e-mail because Jeffrey loves it,” explains the vice president of feature publicity at Disney, before helpfully clarifying: “You communicate by computer instead of by phone.”</p>
			<p class="x04-body-text">Email was still new enough in 1992 that not everyone understood its potential. “E-mail is fun, but it’s a toy,” says a story analyst at Columbia Pictures, providing a quote he probably now wishes he could take back. He then adds: “E-mail encourages people to chatter and say things that don’t need to be said.” The article also notes that at this point, most motion picture studios still depended on a primitive communication device called the Amtel, a combination of screen and keyboard that was used to send short text messages. (A common use of the Amtel in Hollywood was to allow assistants to inform executives, without interrupting their closed-door meetings, about who was holding on various phone lines.)</p>
			<p class="x04-body-text">In a 1989 article, the venerable technology writer John Markoff provides more insight into the dynamics that helped accelerate email’s growth.<a href="19_Notes.xhtml#EndnoteNumber72" id="SuperscriptNumber72" class="calibre3"><sup class="endnote">5</sup></a> “Electronic mail, which has taken a secondary position to the facsimile machine through the personal computer boom of the 1980’s,” he writes, “is finally coming into its own.” As Markoff’s piece clarifies, in the late 1980s, email was largely used to connect employees within the same company. In 1989, under pressure from the Aerospace Industries Association (a group of fifty aerospace companies with over six hundred thousand total employees), the main email network providers “grudgingly” agreed to interconnect their networks using an early email protocol called X.400, allowing users from one network, for the first time, to communicate with users from another.</p>
			<p class="x04-body-text">Markoff presciently argues that once email becomes global, it will <span epub:type="pagebreak" id="page_68" role="doc-pagebreak" title="68"></span>largely eliminate the need for fax machines and therefore spread rapidly. He wasn’t the only one to see this potential. In the article, Markoff quotes Steve Jobs—identified as “Steven P. Jobs”—providing what turned out to be an accurate prediction: “In the 1990’s, personal computing will transform personal communication roughly by the same magnitude that, in the 1980’s, spreadsheets transformed business analysis and desktop publishing.”</p>
			<p class="x04-body-text">The case studies in Markoff’s long piece paint a picture of a technology on the rise. “We found that electronic mail dramatically improved the way in which we communicated,” explains a hospital executive. “It took off and permeated our organization.” Markoff later elaborates: “In large and small offices throughout the country, [email] is being seized on as a means of communication more efficient than the telephone.”</p>
			<p class="x04-body-text">By 1992, the <i class="calibre2">Times</i> reported that email had become a $130-million-a-year business, projected to be a $500 million business by mid-decade as many big software companies, including IBM and Microsoft, began preparing to enter the market.<a href="19_Notes.xhtml#EndnoteNumber73" id="SuperscriptNumber73" class="calibre3"><sup class="endnote">6</sup></a> A couple of years later, email’s dominance was unquestioned. “Ever since the Lotus 1-2-3 spreadsheet was anointed a decade ago as the first killer app . . . people have been asking, ‘What’s the next killer app?’” writes Peter Lewis in a 1994 article. “In my mind, there is no doubt: electronic mail is the killer app for the 1990s.”<a href="19_Notes.xhtml#EndnoteNumber74" id="SuperscriptNumber74" class="calibre3"><sup class="endnote">7</sup></a></p>
			<p class="x04-body-text">As portrayed by these articles, the speed with which email spread through the business sector is astounding. In 1987, it’s a clunky tool useful to only a “niche market.” By 1994, it’s the “killer app” of the decade and the foundation of a half-billion-dollar software industry. That’s about as close to an overnight transformation as you’re likely to find in the history of commercial technology adoption.</p>
			<p class="x04-body-text">We shouldn’t be surprised that this tool spread so fast. As I established, it solved a real problem—the need for high-speed asynchronous <span epub:type="pagebreak" id="page_69" role="doc-pagebreak" title="69"></span>communication—and did so in a manner that was relatively inexpensive and easy to master.<a href="19_Notes.xhtml#EndnoteNumber75" id="SuperscriptNumber75" class="calibre3"><sup class="endnote">8</sup></a> But it’s important to remember that there’s nothing fundamental about email as a tool that demands that we use it constantly. One could imagine an alternative history in which email simplified existing communication that used to occur over voicemail and memos, but office work otherwise remained the same as it had been in the mid-1980s. You can enjoy the practical benefits of email, in other words, without having to also embrace the hyperactive hive mind workflow. So why did this frenetic behavior become universal in the aftermath of email’s arrival, even though, as argued in the preceding chapters, it makes us less productive and more miserable? When you look closer at this question, a nuanced and fascinating collection of answers emerges, all of which point to a surprising conclusion: maybe the way we work today is much more arbitrary than we realize.</p>
			<h3 class="x05-head-a">What Does Technology Want?</h3>
			<p class="x03-co-body-text">Adrian Stone’s first job after graduating from college in the early 1980s was at the IBM headquarters in Armonk, New York. At the time, internal communication at IBM relied heavily on scribbling notes. As Stone recalls in a 2014 essay he wrote about this period, if you wanted to talk to someone, you might try calling, but because this often failed, the default approach was to walk to their cubicle and leave a note for them to read later. “Once they read their little note, they got a chance to be ‘it’ and play the game in reverse,” Stone wrote. “This could go on for days.”<a href="19_Notes.xhtml#EndnoteNumber76" id="SuperscriptNumber76" class="calibre3"><sup class="endnote">9</sup></a></p>
			<p class="x04-body-text">This is an important reminder that the world before email was no prelapsarian paradise. Communicating in big organizations during this period was a real pain, and email, when it came on the scene, offered a simple solution. So it’s no surprise that as IBM began to network its operations in the 1980s, it was quick to deploy an internal email service. <span epub:type="pagebreak" id="page_70" role="doc-pagebreak" title="70"></span>One of Stone’s first tasks at the company was to help these efforts by investigating how much IBM employees at the Armonk headquarters were currently communicating through voicemail, memos, scribbled notes, and so on. They assumed most of this communication would shift to email, and they wanted to provision a sufficiently large mainframe to handle the load. (As Stone explained to me, these machines were expensive at the time—“We’re talking prices in the millions”—so it was important to identify exactly how much processing power you really needed.)</p>
			<p class="x04-body-text">Stone soon put together an estimate for a server that could easily handle all the analog communication already occurring in the office. The system was configured and deployed, and once activated, it was a hit among employees; too much of a hit, as it turned out. Within a few days, they “blew” the server due to overload. As Stone told me, they experienced five to six times <i class="calibre2">more</i> traffic than he had estimated, meaning that almost immediately after email was introduced at IBM, the volume of internal communication exploded.</p>
			<p class="x04-body-text">A closer examination revealed that not only did people send many more messages than they did in the pre-email era; they also began cc’ing these messages to many more people. “Pre-email, simple communication was largely person-to-person,” Stone told me. After email, these same conversations now unfolded over long back-and-forth threads including many different people. “Thus—in a mere week or so—was gained and blown the potential productivity gain of email,” he joked.</p>
			<p class="x04-body-text">This story is important because it highlights a dynamic between people and technology that’s often overlooked. We like to believe that we deploy tools rationally to solve specific problems. But cases like IBM’s server meltdown complicate this story line. No group of managers at IBM decided that massively increasing internal communication would improve productivity, and the individuals suddenly trapped in this <span epub:type="pagebreak" id="page_71" role="doc-pagebreak" title="71"></span>deluge of messages weren’t happy about it. As Adrian Stone recalls, the intention of the system was simply to move the communication that already existed in the office to a more efficient medium—to take what people were already doing and make it easier. So who ultimately decided that everyone should instead start interacting five to six times more than normal? To some who study this question closely, the answer is radical: it was the technology itself.</p>
			<hr class="transition"/>
			<div aria-hidden="true" class="x04-space-break-orn">—</div>
			<p class="x03-co-body-text">If you talk with a scholar of the history of technology, you’ll likely discover a fascination with a seemingly unlikely topic: the rise of medieval feudalism in the early Carolingian Empire. Historians trace the origins of this style of government to the reign of Charles Martel, grandfather to Charlemagne. In the eighth century CE, Martel kick-started feudalism by confiscating Church lands and redistributing them to his vassals.</p>
			<p class="x04-body-text">Why did Martel begin grabbing Church lands? This question was answered in a magisterial tract published in 1887 by the German historian Heinrich Brunner, who argued that granting land to loyal subjects was necessary for Martel to maintain horse-mounted warriors for his army.<a href="19_Notes.xhtml#EndnoteNumber77" id="SuperscriptNumber77" class="calibre3"><sup class="endnote">10</sup></a> In later periods of history, rulers might simply tax their subjects and use the revenue to fund their military, but in the early medieval period, land was the primary source of capital. If you wanted someone to maintain a mounted warrior for your army, they needed land to do so. Brunner marshaled historic documents to demonstrate persuasively that this maintenance of knights in shining armor was one of the main motivations for Martel’s setting up fiefdoms throughout his kingdom.</p>
			<p class="x04-body-text">As is often the case with history, this answer leads to another question: Why did Martel feel the sudden need to raise a massive cavalry <span epub:type="pagebreak" id="page_72" role="doc-pagebreak" title="72"></span>force? Brunner proposed a simple answer. When the Franks under Martel faced a Muslim army from Spain, near Poitiers in 732, Martel’s forces were largely fighting on foot, while the Muslim soldiers were largely mounted. According to Brunner’s theory, Martel quickly realized his disadvantage. Almost immediately after this conflict—indeed, later that same year—he began his sudden confiscation of Church lands. As historian Lynn White Jr. summarizes: “Thus, Brunner concluded, the crisis which generated feudalism, the event which explains its almost explosive development toward the middle of the eighth century, was the Arab incursion.” This theory proved resilient in the decades after it was proposed, standing up, according to White, “remarkably well against assaults from all directions.”<a href="19_Notes.xhtml#EndnoteNumber78" id="SuperscriptNumber78" class="calibre3"><sup class="endnote">11</sup></a></p>
			<p class="x04-body-text">But then in the mid-twentieth century, Brunner’s theory took a blow. New scholarship revealed that Brunner’s date for the pivotal Battle of Poitiers was wrong; it actually took place a year <i class="calibre2">after</i> Martel began grabbing Church lands. “We are faced, in the reigns of Martel [and his successors], with an extraordinary drama which lacks motivation,” writes White.<a href="19_Notes.xhtml#EndnoteNumber79" id="SuperscriptNumber79" class="calibre3"><sup class="endnote">12</sup></a> The idea that feudalism was instigated by the need to support mounted warriors remained an accepted hypothesis, but the reason for this shift toward cavalry was suddenly once again shrouded in mystery. That is, until White, at the time a middle-aged history professor at UCLA, came across a “rambling” footnote, written by a scholar of German antiquities in 1923, that concludes with the following offhanded claim: “The new age is heralded in the eighth century by excavations of stirrups.”<a href="19_Notes.xhtml#EndnoteNumber80" id="SuperscriptNumber80" class="calibre3"><sup class="endnote">13</sup></a></p>
			<p class="x04-body-text">The footnote implied that the force that drove Charles Martel to develop feudalism was the arrival in western Europe of a basic technology: the horse stirrup. In his now classic 1962 book that fills out this hypothesis, <i class="calibre2">Medieval Technology and Social Change</i>, White meticulously <span epub:type="pagebreak" id="page_73" role="doc-pagebreak" title="73"></span>draws from both archaeology and linguistics to show that the introduction of the stirrup does indeed explain well Martel’s sudden shift toward mounted troops.<a href="19_Notes.xhtml#EndnoteNumber81" id="SuperscriptNumber81" class="calibre3"><sup class="endnote">14</sup></a></p>
			<p class="x04-body-text">Before the stirrup, a warrior on a horse had to wield his spear or sword with “the strength of shoulder and biceps.”<a href="19_Notes.xhtml#EndnoteNumber82" id="SuperscriptNumber82" class="calibre3"><sup class="endnote">15</sup></a> The stirrup enabled a “vastly more effective mode of attack.” By bracing a lance between his upper arm and body, a rider leaning forward in metal stirrups could deliver a blow with the combined force of his weight and the weight of his stallion. The difference between these two attacks was monumental. In the eighth century, the warrior with a lance and stirrups on a horse was a form of “shock warfare” devastating to opponents. In a medieval version of the nuclear arms race that would follow more than a millennium later, Charles Martel realized that the advantage provided by the stirrup was so “immense” that he had to do whatever it took to get it before his enemies did—even if that meant upending centuries of tradition and creating a brand-new form of government.</p>
			<p class="x04-body-text">In Lynn White Jr.’s study of the stirrup we find a classic example of a technology introduced for a simple reason (to make riding horses easier) leading to vast and complicated consequences never imagined by its inventors (the rise of medieval feudalism). In the second half of the twentieth century, many scholars in the field of the philosophy of technology began to research similar case studies of unintended consequences. Over time, this idea that tools can sometimes drive human behavior became known as <i class="calibre2">technological determinism.</i></p>
			<p class="x04-body-text">The literature on this philosophy is filled with fascinating examples. One of the better-known determinist books is Neil Postman’s 1985 classic, <i class="calibre2">Amusing Ourselves to Death.</i> In this short treatise, Postman argues that the format through which mass media is delivered can impact the way a culture thinks about the world. (If this reminds you of <span epub:type="pagebreak" id="page_74" role="doc-pagebreak" title="74"></span>Marshall McLuhan’s famed claim that “the medium is the message,” you won’t be surprised to learn that Postman studied under McLuhan.)</p>
			<p class="x04-body-text">Postman uses this concept to argue, among other points, that the impact of the printing press is deeper than we realize. The standard narrative about this invention is that mass-produced pamphlets and books allowed information to spread faster and farther, speeding up the evolution of knowledge that culminated in the Age of Reason. Postman replies that the influence of the resulting “typographic” culture did more than just speed up information flow; it changed the way our brains processed our world. “Print put forward a definition of intelligence that gave priority to the objective, rational use of the mind,” he writes, “and at the same time encouraged forms of public discourse with serious, logically ordered content.”<a href="19_Notes.xhtml#EndnoteNumber83" id="SuperscriptNumber83" class="calibre3"><sup class="endnote">16</sup></a> It was this new way of thinking—not just newly available information—that suddenly made intellectual innovations such as Enlightenment philosophy and the scientific method natural next steps. Gutenberg, in other words, thought he was setting information free, but in reality, he was changing fundamentally what information we treated as important.</p>
			<p class="x04-body-text">A more modern example of technological determinism is the introduction of the Like button to Facebook. As revealed by contemporaneous blog posts written by the design team, the original purpose of this feature was to clean up the comments below users’ posts. Facebook engineers noticed that many such comments were simple positive exclamations, like “cool” or “nice.” They figured that if those could instead be captured by clicking Like, the comments that remained would be more substantive. The goal of this tweak, in other words, was a modest improvement, but they soon noticed an unexpected side effect: users began spending more time on the service.</p>
			<p class="x04-body-text">As became clear in retrospect, incoming Likes provide users with an uneven stream of <i class="calibre2">social approval indicators</i>—bits of evidence that other <span epub:type="pagebreak" id="page_75" role="doc-pagebreak" title="75"></span>people are thinking about you. The idea that every tap of the Facebook app might give you new information about these indicators hijacked ancient social drives in the human brain and made the platform suddenly significantly more appealing. Whereas people used to log on to Facebook occasionally to see what their friends were up to, they were now more likely to check in constantly throughout the day to see how much approval their latest posts had generated. Soon every other major platform introduced similar approval indicator streams—favorites, retweets, auto-tagging photos, streaks—as part of a technological contest played out on the field of what became known as attention engineering, a battle that left in its wake a small number of massively powerful technology platform monopolies and a weary populace exhausted by a life increasingly dominated by handheld glowing screens. All this because of a small number of engineers who desired to make social media comments less cluttered.<a href="19_Notes.xhtml#EndnoteNumber84" id="SuperscriptNumber84" class="calibre3"><sup class="endnote">17</sup></a></p>
			<hr class="transition"/>
			<div aria-hidden="true" class="x04-space-break-orn">—</div>
			<p class="x03-co-body-text">A key property of technological determinism is that the innovation in question alters our behavior in ways that were neither intended nor predicted by those first adopting the tool. This idea might make you uncomfortable, as it seems to impart some notion of autonomy to inanimate objects—as if the technology itself is deciding how it should be used. You wouldn’t be alone in your unease: there are many scholars today who steer well clear of determinist analysis, which in recent years has fallen out of fashion in academic circles currently more enamored with theories that understand tools as vectors of social power. But the longer I study the intersection of technology and office culture, the more I’m convinced that in this particular setting, the determinists have something useful to teach us.</p>
			<p class="x04-body-text">To make this case, let’s first strip this philosophy of its spooky <span epub:type="pagebreak" id="page_76" role="doc-pagebreak" title="76"></span>undertones of self-aware tools. When examined closer, the unintended consequences in technological determinism case studies almost always have pragmatic causes. New tools open up some new options for behavior while closing off others. When these changes then interact with our inscrutable human brains and the complex social systems in which we operate, the results can be both significant and unpredictable. The technologies in question in these studies are not literally deciding how humans should behave, but their effects can be so surprising and sudden to those involved that a story line of tools determining behavior seems as valid as any for describing what’s going on. (The technology scholar Doug Hill uses the term <i class="calibre2">de facto autonomy</i> to describe this effect.)</p>
			<p class="x04-body-text">If you’re careful, you can often look backward after a new tool has created profound change and decode some of the forces at play. In the case of the horse stirrup, for example, scholars have done exactly this work by excavating the specific context in which Charles Martel encountered the stirrup—what was going on in his political world, what experience he had previously with mounted warfare, and so on. In retrospect, the idea that the stirrup would spark feudalism makes sense. But no one planned or predicted it in advance.</p>
			<p class="x04-body-text">This brings us back to email. The case study of Adrian Stone and IBM is pure technological determinism: a tool introduced for a simple purpose (to make existing communication practices more efficient) had an unexpected result (a shift toward the hyperactive hive mind style of collaboration). The speed of this transformation, which required less than a week to get rolling, underscores how powerful these forces can be once unleashed.</p>
			<p class="x04-body-text">Determinist dynamics similar to what Adrian Stone observed at IBM went on to unfold in offices around the world as email spread <span epub:type="pagebreak" id="page_77" role="doc-pagebreak" title="77"></span>throughout the 1990s, ushering in a general embrace of the hyperactive hive mind without anyone ever stopping to ask whether this radical new way of working made any sense. We chose to use email because it was a rational solution to the need for practical asynchronous communication in large offices. The hyperactive hive mind, in some sense, subsequently chose us once this tool had spread, at which point we seemed to have all looked up from our newly empowered inboxes, shrugged, and quipped: “I guess this is how we work now.”</p>
			<h3 class="x05-head-a">Stumbling into the Hive Mind</h3>
			<p class="x03-co-body-text">The horse stirrup enabled a new type of shock troop that the Carolingian Empire couldn’t survive without. This led to land grabs, which in turn upended the very nature of government, and thus we get from the introduction of a narrowly useful bit of metal and leather to full-blown feudalism. I just argued that more than a millennium later, the introduction of another narrowly useful innovation, electronic messaging, led the modern office to embrace the hyperactive hive mind workflow. To justify this claim, let’s look closer at the types of underlying complex forces that plausibly might have driven us from the rational adoption of email to the less rational embrace of the hive mind approach to work. There are at least three of these hive mind drivers that likely played a role in this unintentional transformation of the office.</p>
			<h4 class="x05-head-b">Hive Mind Driver #1: The Hidden Costs of Asynchrony</h4>
			<p class="x03-co-body-text">As argued earlier, email helped solve a practical problem generated by the growing size of offices: the need for efficient asynchronous communication—that is, a fast way to send messages back and forth <span epub:type="pagebreak" id="page_78" role="doc-pagebreak" title="78"></span>without requiring the sender and receiver to be communicating at the same time. Instead of having to play phone tag with a colleague from the other side of your office building, you can replace this real-time conversation with a short message, delivered when convenient for you, and then read when convenient for the recipient.</p>
			<p class="x04-body-text">To many, this asynchronous approach to communication seemed strictly more efficient. One technology commenter I came across in my research compares synchronous communication—the type that requires actual conversation—to an outdated office technology like the fax machine: it’s a relic, he writes, that “will puzzle your grandkids” when they look back on how people used to work.<a href="19_Notes.xhtml#EndnoteNumber85" id="SuperscriptNumber85" class="calibre3"><sup class="endnote">18</sup></a></p>
			<p class="x04-body-text">The problem, of course, is that email didn’t live up to its billing as a productivity silver bullet. The quick phone call, it turns out, cannot always be replaced with a single quick message, but instead often requires dozens of ambiguous digital notes passed back and forth to replicate the interactive nature of conversation. If you multiply the many formerly real-time exchanges now handled through multitudinous messaging, you get a long way toward understanding why the average knowledge worker sends and receives 126 emails per day.<a href="19_Notes.xhtml#EndnoteNumber86" id="SuperscriptNumber86" class="calibre3"><sup class="endnote">19</sup></a></p>
			<p class="x04-body-text">Not everyone, however, was surprised by the added complexity of drawn-out communication. As email was taking over the modern office, scholars in the theory of distributed systems—the subfield of computer science that I study in my academic research—were also examining the trade-offs between synchrony and asynchrony. As it happens, the conclusion they reached was exactly the opposite of the prevailing consensus in the workplace.</p>
			<p class="x04-body-text">The synchrony-versus-asynchrony issue is fundamental to the history of computer science. For the first couple of decades of the digital revolution, programs were designed to run on individual machines. Later, with the development of computer networks, programs were written to <span epub:type="pagebreak" id="page_79" role="doc-pagebreak" title="79"></span>be deployed on multiple machines that operated together over a network, creating what are called <i class="calibre2">distributed systems.</i> Figuring out how to coordinate the machines that made up these systems forced computer scientists to confront the pros and cons of different communication modes.</p>
			<p class="x04-body-text">If you connect a collection of computing machines on a network, their communication, by default, will be asynchronous. Machine A sends a message to Machine B, hoping that it will eventually be delivered and processed, but Machine A doesn’t know for sure how long it will be until Machine B reads the message. This uncertainty could be due to many factors, such as the fact that different machines run at different speeds (if Machine B is also running many other unrelated processes, it might take a while until it gets around to checking its queue of incoming messages), unpredictable network delays, and equipment failures.</p>
			<p class="x04-body-text">Writing distributed system algorithms that could handle this asynchrony turned out to be much harder than many engineers originally believed. A striking computer science discovery from this period, for example, is the difficulty of the so-called <i class="calibre2">consensus problem.</i> Imagine that each machine in a distributed system starts an operation, such as entering a transaction into a database, with an initial preference to either proceed or abort. The goal is for these machines to reach a consensus—either all agreeing to proceed or all agreeing to abort.</p>
			<p class="x04-body-text">The simplest solution is for each machine to gather the preferences of its peers and then apply some fixed rule—for example, counting the votes to determine a winner—to decide which preference to adopt. If all the machines gather the same set of votes, they will all adopt the same decision. The problem is that some of the machines might crash before they vote. If that happens, the rest of the group will end up waiting forever to hear from peers that are no longer operating. Because delays are unpredictable in an asynchronous system, the waiting peers don’t know <span epub:type="pagebreak" id="page_80" role="doc-pagebreak" title="80"></span>when they should give up and move on with the votes they’ve already gathered.</p>
			<p class="x04-body-text">At first, to the engineers who studied this problem, it seemed obvious that instead of waiting to learn the preference of every machine, one could just wait to hear from most of them. Imagine, for example, the following rule: if I hear from most machines, and they all want to proceed, then I’ll decide to proceed; otherwise I’ll default to abort, just to be safe. At first glance, this rule seems like it should lead to a consensus, so long as only a small number of machines die. And yet, to the surprise of many people in the field, in a 1985 paper, three computer scientists—Michael Fischer, Nancy Lynch (my doctoral adviser), and Michael Paterson—proved, through a virtuosic display of mathematical logic, that in an asynchronous system, <i class="calibre2">no</i> distributed algorithm could guarantee that a consensus would always be reached, even if it was sure that at most a single computer might crash.<a href="19_Notes.xhtml#EndnoteNumber87" id="SuperscriptNumber87" class="calibre3"><sup class="endnote">20</sup></a></p>
			<p class="x04-body-text">The details of this result are technical,<a href="19_Notes.xhtml#EndnoteNumber88" id="SuperscriptNumber88" class="calibre3"><sup class="endnote">21</sup></a> but its impact on distributed systems was obvious. It made it clear that asynchronous communication complicates attempts to coordinate, and therefore, it’s almost always worth the extra cost required to introduce more synchrony. In the context of distributed systems, the added synchrony explored in the aftermath of this famous 1985 paper took several forms. One heavy-handed solution, used in some early fly-by-wire systems and fault-tolerant credit card transaction processing machines, was to connect the machines on a common electrical circuit, allowing them to operate at the same lockstep pace. This approach eliminates unpredictable communication delays and allows your application to immediately detect if a machine has crashed.</p>
			<p class="x04-body-text">Because these circuits were sometimes complicated to implement, software approaches to adding synchrony also became popular. By leveraging knowledge about message delays and processor speeds, it <span epub:type="pagebreak" id="page_81" role="doc-pagebreak" title="81"></span>turns out that it’s possible to write programs that structure communication into well-behaved rounds, or simulate reliable machines that can help synchronize the actual unreliable machines participating in the system.</p>
			<p class="x04-body-text">This fight against asynchrony ended up playing a crucial role in the rise of the internet age, enabling, among other innovations, the software driving the huge data centers run by such companies as Amazon, Facebook, and Google. In 2013, Leslie Lamport, a major figure in the field of distributed systems, was given the A. M. Turing Award—the highest distinction in computer science—for his work on algorithms that help synchronize distributed systems.<a href="19_Notes.xhtml#EndnoteNumber89" id="SuperscriptNumber89" class="calibre3"><sup class="endnote">22</sup></a></p>
			<p class="x04-body-text">What’s striking about these technical results on asynchrony versus synchrony is how much they diverge from the conclusions of the business thinkers tackling these same issues in the workplace. As we’ve learned, managers in office settings fixated on eliminating the overhead of synchronous communication—the annoyance of phone tag or taking the elevator to a different floor to chat with someone in person. They believed that eliminating this overhead using tools like email would make collaboration more efficient. Computer scientists, meanwhile, came to the opposite conclusion. Investigating asynchronous communication from the perspective of algorithm theory, they discovered that spreading out communication with unpredictable delays introduced tricky new complexities. While the business world came to see synchrony as an obstacle to overcome, computer theorists began to realize that it was fundamental for effective collaboration.</p>
			<p class="x04-body-text">People are different from computers, but many of the forces that complicate the design of asynchronous distributed systems loosely apply to humans attempting to collaborate in the office. Synchrony might be expensive to arrange—both in the office setting and in computer systems—but trying to coordinate in its absence is also expensive. This <span epub:type="pagebreak" id="page_82" role="doc-pagebreak" title="82"></span>reality summarizes well what many experienced as office communication shifted to email: they traded the pain of phone tag, scribbled notes, and endless meetings for the pain of a surprisingly large volume of ambiguous electronic messages passed back and forth throughout the day. As the engineers discovered when they tried to coax their networked computers into reaching a consensus, asynchrony is not just synchrony spread out; it instead introduces its own difficulties. A problem that might have been solvable in a few minutes of real-time interaction in a meeting room or on the phone might now generate dozens of messages, and even then might still fail to converge on a satisfactory conclusion. It’s possible, in other words, that once you move your workplace toward this style of communication, the <i class="calibre2">hyperactive</i> property of the hyperactive hive mind workflow becomes unavoidable.</p>
			<h4 class="x05-head-b">Hive Mind Driver #2: The Cycle of Responsiveness</h4>
			<p class="x03-co-body-text">Harvard Business School professor Leslie Perlow is an expert in the culture of constant connectivity that dominates the modern workplace. As she recounts in her 2012 book, <i class="calibre2">Sleeping with Your Smartphone</i>, the severity of this problem was brought to her attention by a series of surveys she conducted between 2006 and 2012—the period in which the hive mind workflow shifted into a new gear of hyperactivity as smartphones became common. These surveys targeted over 2,500 managers and professionals who held what Perlow describes as “high-pressure, demanding jobs.”<a href="19_Notes.xhtml#EndnoteNumber90" id="SuperscriptNumber90" class="calibre3"><sup class="endnote">23</sup></a> She asked respondents about their work habits: how many hours they worked per week, how often they checked their work accounts outside work, whether they slept with their phones nearby. The results were stark: these professionals were almost always “on.”</p>
			<p class="x04-body-text">What makes Perlow’s work particularly relevant to our discussion <span epub:type="pagebreak" id="page_83" role="doc-pagebreak" title="83"></span>is that she then went deeper, talking with her research subjects to better understand <i class="calibre2">how</i> they ended up in this state of constant communication. What she uncovered was a social feedback loop gone awry—a process she named the <i class="calibre2">cycle of responsiveness.</i> The cycle begins with legitimate demands on your time. Perhaps it’s 2010, you’ve just started using a smartphone, and you realize it’s now possible to answer client questions that arrive after work hours or respond quickly to colleagues in different time zones. These clients and colleagues now learn that you’re available at these new times and begin to send more requests and expect faster responses. Faced with this increased influx, you check your phone more often so you can keep up with the incoming messages. But now the expectations for your availability and responsiveness increase further, and you feel pressured to respond even quicker. As Perlow summarizes:</p>
			<blockquote class="calibre6">
				<p class="x06-extract-1p">And thus the cycle spins: teammates, superiors and subordinates continue to make more requests, and conscientious employees accept these marginal increases in demands on their time, while their expectations of each other (and themselves) rise accordingly.<a href="19_Notes.xhtml#EndnoteNumber91" id="SuperscriptNumber91" class="calibre3"><sup class="endnote">24</sup></a></p>
			</blockquote>
			<p class="x04-body-text">This is a nice example of technological determinism at work. None of these teammates, superiors, and subordinates <i class="calibre2">like</i> the culture of constant connection that this cycle produces. None of them ever suggested it, or made a conscious decision to adopt it. Indeed, when Perlow later persuaded teams at Boston Consulting Group to schedule protected time away from communication devices, the team members described their efficiency and effectiveness as increasing.<a href="19_Notes.xhtml#EndnoteNumber92" id="SuperscriptNumber92" class="calibre3"><sup class="endnote">25</sup></a> She further proposed an email server configured such that messages sent after work hours <span epub:type="pagebreak" id="page_84" role="doc-pagebreak" title="84"></span>would be automatically held and delivered the next morning (a special flag could be set to bypass this restriction for actual urgent communication). This change might sound simple, but by short-circuiting the cycle of responsiveness, its impact could be profound.</p>
			<p class="x04-body-text">The important lesson from Perlow’s work is the haphazard and unplanned manner in which an entirely new way of communicating emerged. The media theorist Douglas Rushkoff uses the term “collaborative pacing” to describe this tendency for groups of humans to converge toward strict patterns of behavior without ever actually explicitly deciding that the new behaviors make sense.<a href="19_Notes.xhtml#EndnoteNumber93" id="SuperscriptNumber93" class="calibre3"><sup class="endnote">26</sup></a> I notice you’re responding a little quicker to my message, so I begin to do the same. Others follow suit; the pattern of responsiveness emerges, then becomes a new default. The consultants Perlow studied didn’t choose the cycle of responsiveness; in some sense, email chose it for them.</p>
			<h4 class="x05-head-b">Hive Mind Driver #3: The Caveman at the Computer Screen</h4>
			<p class="x03-co-body-text">In a paper published in 2018 in the journal <i class="calibre2">Quaternary</i>, Tel Aviv University archaeologists Aviad Agam and Ran Barkai review the available “archaeological, ethnographic and ethno-historical records” to summarize our current understanding of how early humans, starting in the Lower Paleolithic, hunted elephants and mammoths.<a href="19_Notes.xhtml#EndnoteNumber94" id="SuperscriptNumber94" class="calibre3"><sup class="endnote">27</sup></a> This paper includes four striking charcoal drawings illustrating the authors’ best guesses about how these hunts might have transpired.</p>
			<p class="x04-body-text">The first drawing shows a group of seven Paleolithic hunters charging a rearing elephant, each throwing spears toward vulnerable organs. The second and third show lone hunters attempting to surprise an elephant by stealth, landing a critical spear stab before the animal realizes what is happening. In one case, the hunter attacks from below, stabbing up through the belly; in the other, the hunter hides in a tree and stabs <span epub:type="pagebreak" id="page_85" role="doc-pagebreak" title="85"></span>downward as the elephant passes. In the fourth drawing, a group of six hunters rush to finish off with spears an elephant that has fallen into a pitfall trap.</p>
			<p class="x04-body-text">For our purposes, it’s important to notice the small size of the groups engaged in each of these hunting scenarios. Throughout our species’s deep history, this evidence suggests, when we hunted megafauna we did so either alone or in small groups. This reality likely also holds for the other activities—hunting small game, foraging—that made up the “work” that dominates our evolutionary history. It doesn’t require a large leap of speculative evolutionary psychology to arrive at the reasonable conclusion that <i class="calibre2">Homo sapiens</i> are well adapted to small-group collaboration.</p>
			<p class="x04-body-text">To connect this observation of our deep past to our current discussion of email, consider the dynamics of these collaborations. If you’re part of a small group of Paleolithic hunters stealthily approaching an elephant, your communication would be ad hoc and unstructured as you adjust on the fly to the unfolding situation (imagine the following dialogue delivered in some now lost caveman dialect):</p>
			<blockquote class="calibre6">
				<p class="x06-dialogue-first">“Careful . . . watch out for those sticks, which might crack and spook the elephant. . . .”</p>
				<p class="x06-dialogue">“Wait, circle around that way. . . .”</p>
				<p class="x06-dialogue-last">“Slowly now, its ears are perking up. . . .”</p>
			</blockquote>
			<p class="x04-body-text">Even when we leave deep history and return to our more recent pre-industrial past, for the vast majority of people, the vast majority of their experience working with others would still involve small groups—from the farmer and his kids navigating a plow to the blacksmith working closely with his apprentice at the forge. As with the Paleolithic <span epub:type="pagebreak" id="page_86" role="doc-pagebreak" title="86"></span>hunters, the most natural way for small groups to coordinate is in a free-form manner. It follows that the mode of collaboration most instinctually embedded in both our genetics and our cultural memory shares the main characteristics of the hyperactive hive mind workflow. We shouldn’t be surprised, therefore, that when the introduction of low-friction messaging tools like email made similarly unstructured communication possible in the modern large office scenario, we were drawn to this mode of interaction.</p>
			<p class="x04-body-text">The problem, of course, is that the hyperactive hive mind deployed in an office differs from the hive mind collaboration of a Stone Age elephant hunt in one key property: the office connects many more people. Unstructured coordination is great for a group of six hunters but becomes disastrously ineffective when you connect many dozens, if not hundreds, of employees in a large organization. We know this in part because of the robust research literature studying the optimal group size for working together and solving professional problems. “The size question has been asked since the dawn of social psychology,” explains Jennifer Mueller, a management professor at Wharton.<a href="19_Notes.xhtml#EndnoteNumber95" id="SuperscriptNumber95" class="calibre3"><sup class="endnote">28</sup></a></p>
			<p class="x04-body-text">One of the first studies in this area was the now famous work of a nineteenth-century French agricultural engineer, Maximilien Ringelmann, who demonstrated that when you dedicate more people to the task of pulling a rope, the average force exerted by each individual decreases—leading to diminishing returns as group sizes grow. Though the physical task of rope pulling is not that relevant to the modern knowledge sector, Ringelmann’s work proved influential, as it introduced the general idea that increasing the size of a team doesn’t necessarily increase its effectiveness in direct proportion.</p>
			<p class="x04-body-text">In the modern era, many management professors have built on this observation by studying what happens to the effectiveness of workplace <span epub:type="pagebreak" id="page_87" role="doc-pagebreak" title="87"></span>collaboration when you increase team sizes. A 2006 review article published by Wharton summarizes many such research papers. Though there’s no specific team size that consistently emerges as optimal, essentially every result falls into a narrow range of roughly four to twelve people—exactly as we observed all the way back with the Paleolithic elephant hunters.</p>
			<p class="x04-body-text">There are many proposed reasons for why teams above this range are less effective. The loafing effect first observed by Ringelmann, for example, seems to still play a role in knowledge work tasks. (Summarized simply: the more people working on a project, the easier it is to get away with putting in less effort.) But another key factor is the rising complexity of communication. It’s easy for six elephant hunters to coordinate their attack by just speaking up when they have something relevant to say. But if you increase this size to sixty, the effort would devolve into an incomprehensible scrum of competing voices and misunderstood ideas, which is why military units of this size almost always feature strict chains of command.</p>
			<p class="x04-body-text">Pulling together these threads, we can weave a compelling narrative that helps explain the hyperactive hive mind’s spread. Throughout most of human history we worked together in small groups, communicating in an ad hoc fashion without any particular structure or rules. The rise of the large office in the early twentieth century completely disrupted these natural modes of collaboration, requiring us instead to send memos to be carbon-copied in typing pools, or have secretaries arrange one-on-one phone calls. When email arrived, we found a way to bring back a more primal mode of communication to our otherwise alienating office environments—we could just talk, on the fly, sending messages as the thoughts arrived, and expect responses promptly: the elephant hunt reenacted over network wires. The result was the <span epub:type="pagebreak" id="page_88" role="doc-pagebreak" title="88"></span>hyperactive hive mind workflow—which made sense at an instinctual level, even while at a practical level it began to drive us toward misery as we misjudged its ability to scale up to large groups.</p>
			<p class="x04-body-text">Put another way, although the now common tableau of the frantic business executives furiously typing on their phones might seem like the personification of our modern moment, it’s perhaps downright Paleolithic in its origins.</p>
			<h3 class="x05-head-a">Peter Drucker and the Tragedy of the Attention Commons</h3>
			<p class="x03-co-body-text">As a child in Austria during the first decades of the twentieth century, Peter Drucker was exposed to some of the foremost economic thinkers of the age, including notables like Joseph Schumpeter of “creative destruction” fame, who attended evening salons held by Drucker’s parents, Adolph and Caroline.<a href="19_Notes.xhtml#EndnoteNumber96" id="SuperscriptNumber96" class="calibre3"><sup class="endnote">29</sup></a> The intellectual energy of these salons laid the foundation for Drucker’s eventual emergence as one of the most important business thinkers of the modern period; he is widely acknowledged as the “founder of modern management.”<a href="19_Notes.xhtml#EndnoteNumber97" id="SuperscriptNumber97" class="calibre3"><sup class="endnote">30</sup></a> His career produced thirty-nine books and countless articles before his death in 2005 at the age of ninety-five.</p>
			<p class="x04-body-text">Drucker’s sprint toward significance first picked up speed in 1942, when, as a thirty-three-year-old professor at Bennington College, he published his second book, <i class="calibre2">The Future of Industrial Man.</i> It asked how an “industrial society”—one unfolding within “the entirely new physical reality Western man has built up as his habitat since James Watt invented the steam engine”<a href="19_Notes.xhtml#EndnoteNumber98" id="SuperscriptNumber98" class="calibre3"><sup class="endnote">31</sup></a>—might best be structured to respect human freedom and dignity. Arriving in the midst of an industrial world war, the book found a wide readership. It impressed the management team at General Motors, who invited Drucker to spend two years studying <span epub:type="pagebreak" id="page_89" role="doc-pagebreak" title="89"></span>how the world’s largest corporation operated.<a href="19_Notes.xhtml#EndnoteNumber99" id="SuperscriptNumber99" class="calibre3"><sup class="endnote">32</sup></a> The 1946 title that resulted from this engagement, <i class="calibre2">Concept of the Corporation</i>, was one of the first books to look seriously at how big organizations actually operated. It laid the foundation for management as something that could be studied, and it made Drucker’s career.</p>
			<p class="x04-body-text">For our purposes, Drucker is more than just a famous business theorist. His influence also helps answer a pressing question that likely snagged your attention as you read this chapter: Even if we accept that the hyperactive hive mind arose largely of its own accord, why did we let it stick around once its flaws became obvious?</p>
			<hr class="transition"/>
			<div aria-hidden="true" class="x04-space-break-orn">—</div>
			<p class="x03-co-body-text">During his time at GM in the 1940s, Peter Drucker got to know its larger-than-life CEO, Alfred P. Sloan Jr. As Drucker later recalled, Sloan once said the following about being a successful manager: “He must be absolutely tolerant and pay no attention to how a man does his work.”<a href="19_Notes.xhtml#EndnoteNumber100" id="SuperscriptNumber100" class="calibre3"><sup class="endnote">33</sup></a> This idea re-emerged in Drucker’s thinking in the 1950s and 1960s, a period in which he coined the term <i class="calibre2">knowledge work</i> as he began to grapple with an emerging economy where the output of brains was beginning to prove more valuable than the output of factories.</p>
			<p class="x04-body-text">“The knowledge worker cannot be supervised closely or in detail,” Drucker wrote in his 1967 book, <i class="calibre2">The Effective Executive.</i> “He must direct himself.”<a href="19_Notes.xhtml#EndnoteNumber101" id="SuperscriptNumber101" class="calibre3"><sup class="endnote">34</sup></a> This was a radical idea. In the nation’s factories, centralized control of workers was the standard. Influenced by the so-called “scientific management” principles popularized by Frederick Winslow Taylor, who would famously prowl the factory floor with a stopwatch, rooting out inefficient movements, industrial management saw workers as automatons executing optimized processes carefully designed by a small cadre of wise managers.</p>
			<p class="x04-body-text">Drucker argued this approach was doomed to fail in the new world <span epub:type="pagebreak" id="page_90" role="doc-pagebreak" title="90"></span>of knowledge work, where productive output was created not by expensive equipment stamping out parts, but instead by cerebral workers applying specialized cognitive skills. Indeed, knowledge workers often knew more about their specialties than those who managed them. The best way to deploy these highly skilled individuals, Drucker concluded, was to give them clear objectives and then leave them alone to accomplish their brainy work however they saw fit. While it might have been efficient to tell an assembly line worker exactly how to install a steering wheel, it was futile to try to tell a marketing copywriter exactly how to brainstorm a new product slogan.</p>
			<p class="x04-body-text">Drucker preached this idea of knowledge worker autonomy throughout his long career. As late as 1999, he still emphasized its importance:</p>
			<blockquote class="calibre6">
				<p class="x06-extract-1p">[Knowledge work] demands that we impose the responsibility for their productivity on the individual knowledge workers themselves. Knowledge workers <i class="calibre2">have</i> to manage themselves. They have to have <i class="calibre2">autonomy.</i><a href="19_Notes.xhtml#EndnoteNumber102" id="SuperscriptNumber102" class="calibre3"><sup class="endnote">35</sup></a></p>
			</blockquote>
			<p class="x04-body-text">It’s hard to overestimate the influence of this idea. With the exception of some routinized bureaucratic processes, like filing expense reports, the intricacies of how the myriad demanding tasks that define modern office work are accomplished remain largely beyond the scope of management. They’re pushed instead into the hazy realm of personal productivity. Want to know how to get things done? Buy a book on how to better organize your tasks (Drucker himself wrote one of the first such books, <i class="calibre2">The Effective Executive</i>), or use a new planner, or, as is more commonly suggested in our culture of “crushing it,” simply work harder. Knowledge workers don’t expect their organization to take an interest in how much work falls on their plate, or how they get it done.</p>
			<p class="x04-body-text"><span epub:type="pagebreak" id="page_91" role="doc-pagebreak" title="91"></span>In our shift from industrial to knowledge work, in other words, we gave up automaton status for a burdensome autonomy. It’s in this context that the hyperactive hive mind, once in place, became devilishly difficult to eradicate, as it’s hard to fix a broken workflow when it’s no one’s job to make sure the workflow functions. In 1833, the British economist William Forster Lloyd proposed a hypothetical scenario, now a classical example in game theory, that can help us better understand this dynamic. The scenario, which eventually became known as the <i class="calibre2">tragedy of the commons</i>,<a href="19_Notes.xhtml#EndnoteNumber103" id="SuperscriptNumber103" class="calibre3"><sup class="endnote">36</sup></a> considers a town that maintains common grazing land for cattle and sheep, as was typical in Great Britain in the nineteenth century. Lloyd pointed out an interesting tension: it’s in the individual interest of each herder to graze his animals as much as possible on the commons, and yet when all herders act in their best interest, they’ll inevitably overgraze the commons, rendering it useless to everyone. Similar scenarios of individual interest leading to collective hardship turn out to be common in many different settings—from unstable ecologies, to resource mining, to the behaviors surrounding shared refrigerators. Using the mathematical tools introduced in the mid-twentieth century by John Nash (of <i class="calibre2">A Beautiful Mind</i> fame), you can even precisely analyze this situation, which turns out to be a nice example of what game theorists would call an “inefficient Nash equilibrium.”</p>
			<p class="x04-body-text">This economic trivia informs our discussions here because when the hyperactive hive mind emerged due to the drivers summarized earlier in this chapter, communication in the modern office became yet another example of Lloyd’s thought experiment in action. Once your organization has fallen into the hive mind, it’s in each individual’s immediate interest to stick with this workflow, even if it leads to a bad long-term outcome for the organization as a whole. It makes your life <span epub:type="pagebreak" id="page_92" role="doc-pagebreak" title="92"></span>strictly easier in the moment if you can expect quick responses to messages that you shoot off to colleagues. Similarly, if you unilaterally decrease the time you spend checking your inbox in a group that depends on the hive mind, you’ll slow down other people’s efforts, generating annoyance and dissatisfaction that might put your job in jeopardy. At the risk of stretching this analogy beyond comfort, in knowledge work, we’re overgrazing our common collection of time and attention because none of us wants to be the one who lets their cognitive sheep go hungry.</p>
			<p class="x04-body-text">The negative consequences of the hyperactive hive mind, in other words, are unlikely to be resolved by small shifts in individual habits. Even good-natured attempts to nudge the behavior of an entire organization, such as promulgating better norms around email responsiveness or attempting one-off experiments like email-free Fridays, are doomed to fail. As 150 years of economic theory has taught us, to solve the tragedy of the commons, you cannot expect substantially better behavior from the herders; you need instead to replace the free-for-all grazing system with something more efficient. The same holds for the hyperactive hive mind: we cannot tame it with minor hacks—we need to replace it with a better workflow. And to do so, we must soften Peter Drucker’s stigma against engineering office work. Drucker was right to point out that we cannot fully systematize the specialized efforts of knowledge workers, but we shouldn’t apply this to the workflows that surround these efforts. A manager can’t tell a copywriter how to come up with a brilliant ad, but she can have something to say about how these commissions are assigned, or about what other obligations are allowed onto the copywriter’s plate, or about how client requests are handled.</p>
			<p class="x04-body-text">This goal of putting into place smarter workflows that sidestep the worst impacts of the hyperactive hive mind is of course a substantial <span epub:type="pagebreak" id="page_93" role="doc-pagebreak" title="93"></span>endeavor—one that will require trial and error and many annoyances. But with the right guiding principles it’s absolutely possible, and the competitive advantage it will generate is potentially massive. The second part of this book, at which we have now arrived, is dedicated to explaining these <span epub:type="pagebreak" id="page_94" role="doc-pagebreak" title="94"></span>principles.</p>
		</div>
	


  </div>

  
  <div class="calibreToc">
    <h2><a href="../../../n6gwhr7i.html">Table of contents
</a></h2>
    <div>
  <ul>
    <li>
      <a href="02_Also_by_Cal_Newport.xhtml">Also by Cal Newport</a>
    </li>
    <li>
      <a href="03_Title_Page.xhtml">Title Page</a>
    </li>
    <li>
      <a href="04_Copyright.xhtml">Copyright</a>
    </li>
    <li>
      <a href="05_Dedication.xhtml">Dedication</a>
    </li>
    <li>
      <a href="06_Contents.xhtml">Contents</a>
    </li>
    <li>
      <a href="07_Introduction_The_Hype.xhtml">Introduction: The Hyperactive Hive Mind</a>
    </li>
    <li>
      <a href="08_Part_1_The_Case_Again.xhtml">Part 1: The Case Against Email</a>
      <ul>
        <li>
          <a href="09_Chapter_1_Email_Reduc.xhtml">Chapter 1: Email Reduces Productivity</a>
        </li>
        <li>
          <a href="10_Chapter_2_Email_Makes.xhtml">Chapter 2: Email Makes Us Miserable</a>
        </li>
        <li>
          <a href="11_Chapter_3_Email_Has_a.xhtml">Chapter 3: Email Has a Mind of Its Own</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="12_Part_2_Principles_for.xhtml">Part 2: Principles for a World Without Email</a>
      <ul>
        <li>
          <a href="13_Chapter_4_The_Attenti.xhtml">Chapter 4: The Attention Capital Principle</a>
        </li>
        <li>
          <a href="14_Chapter_5_The_Process.xhtml">Chapter 5: The Process Principle</a>
        </li>
        <li>
          <a href="15_Chapter_6_The_Protoco.xhtml">Chapter 6: The Protocol Principle</a>
        </li>
        <li>
          <a href="16_Chapter_7_The_Special.xhtml">Chapter 7: The Specialization Principle</a>
        </li>
        <li>
          <a href="17_Conclusion_The_Twenty.xhtml">Conclusion: The Twenty-First-Century Moonshot</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="18_Acknowledgments.xhtml">Acknowledgments</a>
    </li>
    <li>
      <a href="19_Notes.xhtml">Notes</a>
    </li>
    <li>
      <a href="20_Index.xhtml">Index</a>
    </li>
    <li>
      <a href="21_About_the_Author.xhtml">About the Author</a>
    </li>
  </ul>
</div>


  </div>
  

  <div class="calibreEbNav">
    
      <a href="10_Chapter_2_Email_Makes.xhtml" class="calibreAPrev">previous page
</a>
    

    <a href="../../../n6gwhr7i.html" class="calibreAHome">start
</a>

    
      <a href="12_Part_2_Principles_for.xhtml" class="calibreANext">next page
</a>
    
  </div>

</div>

</body>
</html>
